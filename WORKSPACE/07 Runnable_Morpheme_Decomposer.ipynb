{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Runnable Morpheme Decomposer Model\n",
    "\n",
    "This Notebook is a **runnable Hungarian Morpheme Decomposer**. The necessary files are available in the same folder. These include multiple dataframes and Keras H5 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Importing the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "random_state = 777\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Loading the dataframes for set-similarity recognition, appending additional words, and sorting the lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\AMD\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\AMD\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\AMD\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Large dropout rate: 0.57 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "adverbs = pd.read_csv('adverbs.csv')\n",
    "adverbs_list = list(adverbs['word'])\n",
    "\n",
    "def_arts_list = ['a', 'az']\n",
    "indef_arts_list = ['egy']\n",
    "\n",
    "conjs = pd.read_csv('conjs.csv')\n",
    "conjs_list = list(conjs['word'])\n",
    "\n",
    "dets = pd.read_csv('dets.csv')\n",
    "dets_list = list(dets['word'])\n",
    "new_dets = 'egyugyanaz egyugyanez egyugyanezen egyugyanakként egyugyanekként ugyanaz ugyanez ugyanakként ugyanekként mindez e ugyanakkor ugyanekkor egyugyanakkor egyugyanekkor'.split(' ')\n",
    "dets_list += new_dets\n",
    "dets_list.sort()\n",
    "\n",
    "nums_aggreg = pd.read_csv('nums_aggreg.csv')\n",
    "nums_aggreg_list = list(nums_aggreg['word'])\n",
    "new_nums_aggreg = 'akármennyien ketten hárman négyen öten hatan heten nyolcan kilencen tizen tizenegyen huszonhatan harminckilencen negyvenöten ötvenheten hatvanketten hetvennyolcan nyolcvannégyen kilencvenhárman sokszázan sokezren sokszázezren sokmillióan sokszázmillióan százmilióan sokmillárdan sokbillióan sokbilliárdan billiárdan billióan soktrillióan soktrilliárdan trilliárdan trillióan páran jópáran'.split(' ')\n",
    "nums_aggreg_list += new_nums_aggreg\n",
    "nums_aggreg_list = list(set(nums_aggreg_list))\n",
    "nums_aggreg_list.sort()\n",
    "\n",
    "nums_multipl = pd.read_csv('nums_multipl.csv')\n",
    "nums_multipl_list = list(nums_multipl['word'])\n",
    "new_nums_multipl = 'kétszerte négyszerte hétszerte kilencszerte százszorta ezerszerte miliószorta millárdszorta billiószorta billiárdszorta trilliószorta trilliárdszorta ötvenhetedszerte nyolcvankilencedszerte tizennegyedszerte hatvanegyedszerte harmincötödszörte hetvenkettedszerte negyvenhatdoszorta huszonharmadszorta kilencvennyolcadszorta'.split(' ')\n",
    "nums_multipl_list += new_nums_multipl\n",
    "nums_multipl_list.sort()\n",
    "\n",
    "nums_iter = pd.read_csv('nums_iter.csv')\n",
    "nums_iter_list = list(nums_iter['word'])\n",
    "new_nums_iter = 'előszörre elsőre negyedszerre ötödszörre hetedszerre nyolcadszorra kilencedszerre tizedszerre századszorra ezredszerre milliárdadszorra billomodszorra billiárdadszorra trilliomodszorra trilliárdadszorra negyvenhatodszorra kilencvenharmadszorra huszonegyedszerre tizennegyedszerre nyolcvankettedszerre hetvenhetedszerre hatvankilencedszerre ötvennyolcadszorra harmincötödszörre'.split(' ')\n",
    "nums_iter_list += new_nums_iter\n",
    "nums_iter_list.sort()\n",
    "\n",
    "nums = pd.read_csv('nums.csv')\n",
    "nums.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "nums_list = list(set(nums['stem']))\n",
    "new_nums = 'kilencvenöt nyolcvannégy ötvenhét harvanhárom huszonhat harmicnyolc tizenegy hetvenkilenc negyvenkettő sokszáz sokezer sokszázezer sokmilió sokmilliárd billió sokbillió billiárd sokbilliárd trillió soktrillió trilliárd soktrilliárd többszáz többezer többmillió többmilliárd többtrillió többtrilliárd'.split(' ')\n",
    "nums_list += new_nums\n",
    "nums_list.sort()\n",
    "\n",
    "nums_order_list = 'első második harmadik negyedik ötödik hatodik hetedik nyolcadik kilencedik tizedik hányadik akárhányadik valahányadik sokadik többedik negyvenkettedik ötvenegyedik tizenkilencedik hetvenötödik huszonnegyedik hatvanharmadik harminchatodik kilencvennyolcadik nyolcvanhetedik századik sokszázadik többszázadik ezredik sokezredik többezredik milliomodik sokmilliomodik többmilliomodik millárdadik sokmillárdadik többmillárdadik billiomodik sokbilliomodik billárdadik sokbillárdadik trilliomodik soktrilliomodik többtrilliomodik trillárdadik soktrillárdadik többtrillárdadik'.split(' ')\n",
    "\n",
    "nums_conv_model = load_model('nums_conv_model_extended2.h5')\n",
    "\n",
    "onos = pd.read_csv('onos.csv')\n",
    "onos_list = list(onos['word'])\n",
    "\n",
    "postps = pd.read_csv('postps.csv')\n",
    "postps_list = list(set(postps['word']))\n",
    "postps_list.sort()\n",
    "\n",
    "preps = pd.read_csv('preps.csv')\n",
    "preps_list = list(preps['word'])\n",
    "preps_list += ['pro']\n",
    "\n",
    "prevs = pd.read_csv('prevs.csv')\n",
    "prevs_list = list(set(prevs['word']))\n",
    "prevs_list.sort()\n",
    "\n",
    "utt_ints = pd.read_csv('utt_ints.csv')\n",
    "utt_ints_list = list(set(utt_ints['word']))\n",
    "utt_ints_list.sort()\n",
    "\n",
    "nums_extended = pd.read_csv('nums_extended.csv')\n",
    "all_nums_list = list(nums_extended['word'])\n",
    "all_nums_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** This codeblock contains multiple useful functions:\n",
    "\n",
    "- An encoder and decoder for the Neural Network inputs.\n",
    "- Word-similarity and set-similarity functions for wordtypes that only contain a few unique words.\n",
    "- Morpheme decomposer functions for adjectives, determiners, nouns, numbers and verbs. These functions use trained Keras models for prediction.\n",
    "- A word type predictor function, which also uses a trained Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.57 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.57 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.57 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.57 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "chars = \" 0123456789.:,;!%&'*_-=~\\\\()|[]{}aáäbcdeéfghiíjklmnoóöőpqrstuúüűvwxyz\"\n",
    "encode_dict = {}\n",
    "decode_dict = {}\n",
    "\n",
    "for c in range(len(chars)):\n",
    "    encode_dict[chars[c]] = c\n",
    "    decode_dict[c] = chars[c]\n",
    "\n",
    "def encode(w):\n",
    "    ret = []\n",
    "    for c in w:\n",
    "        ret.append(encode_dict[c])\n",
    "    return np.array(ret)\n",
    "\n",
    "def decode(a):\n",
    "    ret = []\n",
    "    for i in a:\n",
    "        ret.append(decode_dict[i])\n",
    "    return ''.join(ret)\n",
    "\n",
    "M = len(encode_dict)\n",
    "W = 44\n",
    "def one_hot_encode(w):\n",
    "    e = encode(w)\n",
    "    ohe = np.zeros((W, M))\n",
    "    ohe[np.arange(len(e)),e] = 1\n",
    "    return ohe \n",
    "\n",
    "def one_hot_encode_col(col):\n",
    "    return np.array([one_hot_encode(w) for w in col])\n",
    "\n",
    "def word_similarity(w1, w2):\n",
    "    return nltk.edit_distance(w1, w2) / max(len(w1),len(w2))\n",
    "\n",
    "def set_similarity(word, word_set, word_set_avg_len=None):\n",
    "    sim_full_sum = 0\n",
    "    sim_end_sum = 0\n",
    "    counter = 0\n",
    "    \n",
    "    if word_set_avg_len is not None:\n",
    "        avg_len = word_set_avg_len\n",
    "    else:\n",
    "        avg_len = int(np.mean([len(w) for w in word_set]))\n",
    "\n",
    "    for w in word_set:\n",
    "        sim_full = 1 - word_similarity(word, w)\n",
    "        sim_end = 1 -  word_similarity(word[max(0,len(word)-avg_len):], w)\n",
    "        if sim_full > 0.5:\n",
    "            counter += 1\n",
    "            sim_full_sum += sim_full\n",
    "        if sim_end > 0.5:\n",
    "            counter += 1\n",
    "            sim_end_sum += sim_end\n",
    "    \n",
    "    return ((sim_full_sum + sim_end_sum) / 2) * (counter / len(word_set))\n",
    "\n",
    "adjectives_conv_model = load_model('adjectives_conv_model.h5')\n",
    "def morphemes_adjective(adjective):\n",
    "    example = one_hot_encode_col([adjective])\n",
    "    prediction = adjectives_conv_model.predict(example).round()\n",
    "    morpheme_names = ['ANP', 'ANP<PLUR>', 'CAS<ABL>', 'CAS<ACC>', 'CAS<ADE>', 'CAS<ALL>', 'CAS<CAU>', 'CAS<DAT>', 'CAS<DEL>', 'CAS<ELA>', 'CAS<ESS>', 'CAS<FOR>', 'CAS<ILL>', 'CAS<INE>', 'CAS<INS>', 'CAS<SBL>', 'CAS<SUE>', 'CAS<TEM>', 'CAS<TER>', 'CAS<TRA>', 'PLUR', 'PLUR<FAM>', 'POSS', 'POSS<1>', 'POSS<1><PLUR>', 'POSS<2>', 'POSS<2><PLUR>', 'POSS<PLUR>']\n",
    "    return {'stem': adjective[:int(prediction[0][-1])], 'morphemes': [morpheme_names[i] for i in range(len(morpheme_names)) if prediction[0][i] == 1]}\n",
    "\n",
    "determiners_conv_model = load_model('determiners_conv_model2.h5')\n",
    "def morphemes_determiner(determiner):\n",
    "    example = one_hot_encode_col([determiner])\n",
    "    prediction = determiners_conv_model.predict(example).round()\n",
    "    morpheme_names = ['ANP', 'ANP<PLUR>', 'CAS<ABL>', 'CAS<ACC>', 'CAS<ADE>', 'CAS<ALL>', 'CAS<CAU>', 'CAS<DAT>', 'CAS<DEL>', 'CAS<ELA>', 'CAS<FOR>', 'CAS<ILL>', 'CAS<INE>', 'CAS<INS>', 'CAS<SBL>', 'CAS<SUE>', 'CAS<TEM>', 'CAS<TRA>', 'PLUR', 'POSS<2><PLUR>']\n",
    "    return {'stem': determiner[:int(prediction[0][-1])], 'morphemes': [morpheme_names[i] for i in range(len(morpheme_names)) if prediction[0][i] == 1]}\n",
    "\n",
    "nouns_conv_model = load_model('nouns_conv_model3.h5')\n",
    "def morphemes_noun(noun):\n",
    "    example = one_hot_encode_col([noun])\n",
    "    prediction = nouns_conv_model.predict(example).round()\n",
    "    morpheme_names = ['ANP', 'ANP<PLUR>', 'CAS<ABL>', 'CAS<ACC>', 'CAS<ADE>', 'CAS<ALL>', 'CAS<CAU>', 'CAS<DAT>', 'CAS<DEL>', 'CAS<ELA>', 'CAS<ESS>', 'CAS<FOR>', 'CAS<ILL>', 'CAS<INE>', 'CAS<INS>', 'CAS<SBL>', 'CAS<SUE>', 'CAS<TEM>', 'CAS<TER>', 'CAS<TRA>', 'PERS', 'PERS<1>', 'PERS<2>', 'PLUR', 'PLUR<ANP>', 'PLUR<FAM>', 'POSS', 'POSS<1>', 'POSS<1><PLUR>', 'POSS<2>', 'POSS<2><PLUR>', 'POSS<PLUR>', 'POSTP<ALATT>', 'POSTP<ALÁ>', 'POSTP<ALÓL>', 'POSTP<ELLEN>', 'POSTP<ELLENÉRE>', 'POSTP<ELÉ>', 'POSTP<ELÉBE>', 'POSTP<ELŐL>', 'POSTP<ELŐTT>', 'POSTP<FELETT>', 'POSTP<FELÉ>', 'POSTP<FELÜL>', 'POSTP<FELŐL>', 'POSTP<FÖLIBE>', 'POSTP<FÖLÉ>', 'POSTP<FÖLÜL>', 'POSTP<HELYETT>', 'POSTP<IRÁNT>', 'POSTP<KÖRÉ>', 'POSTP<KÖRÖTT>', 'POSTP<KÖRÜL>', 'POSTP<KÖZBEN>', 'POSTP<KÖZIBE>', 'POSTP<KÖZÉ>', 'POSTP<KÖZÖTT>', 'POSTP<KÖZÜL>', 'POSTP<LÉTÉRE>', 'POSTP<MELLETT>', 'POSTP<MELLÉ>', 'POSTP<MELLŐL>', 'POSTP<MIATT>', 'POSTP<MÖGÉ>', 'POSTP<MÖGÖTT>', 'POSTP<MÖGÜL>', 'POSTP<NÉLKÜL>', 'POSTP<RÉSZÉRE>', 'POSTP<RÉSZÉRŐL>', 'POSTP<SZERINT>', 'POSTP<SZÁMÁRA>', 'POSTP<UTÁN>', 'POSTP<VÉGBŐL>', 'POSTP<VÉGETT>', 'POSTP<VÉGRE>', 'POSTP<ÁLTAL>', 'POSTP<ÓTA>']\n",
    "    return {'stem': noun[:int(prediction[0][-1])], 'morphemes': [morpheme_names[i] for i in range(len(morpheme_names)) if prediction[0][i] == 1]}\n",
    "\n",
    "nums_conv_model = load_model('nums_conv_model_extended2.h5')\n",
    "def morphemes_num(num):\n",
    "    example = one_hot_encode_col([num])\n",
    "    prediction = nums_conv_model.predict(example).round()\n",
    "    morpheme_names = ['AGGREG', 'ITER', 'MULTIPL', 'ORDER', 'COUNT']\n",
    "    return {'morphemes': [morpheme_names[i] for i in range(len(morpheme_names)) if prediction[0][i] == 1]}\n",
    "\n",
    "verbs_conv_model = load_model('verbs_conv_model2.h5')\n",
    "def morphemes_verb(verb):\n",
    "    example = one_hot_encode_col([verb])\n",
    "    prediction = verbs_conv_model.predict(example).round()\n",
    "    morpheme_names = ['COND', 'COND<PAST>', 'DEF', 'INF', 'MODAL', 'PAST', 'PERS', 'PERS<1<OBJ<2>>>', 'PERS<1>', 'PERS<2>', 'PLUR', 'SUBJUNC-IMP']\n",
    "    return {'stem': verb[:int(prediction[0][-1])], 'morphemes': [morpheme_names[i] for i in range(len(morpheme_names)) if prediction[0][i] == 1]}\n",
    "\n",
    "word_type_nn = load_model('types_conv_model3.h5')\n",
    "def predict_basic_word_type(w):\n",
    "    nn_input = one_hot_encode_col([w])\n",
    "    return word_type_nn.predict(nn_input)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** And finally, this codeblock accomplishes the word type prediction and morpheme decomposition. The first function is a technicality to output the results in the correct shape. The second function puts everything together from the previous codeblocks.\n",
    "\n",
    "It works by the following idea:\n",
    "\n",
    "- Let's say we gave an input word for which the noun predictor returned with a value of 0.62: this is the likelihood for the given word to be a noun, according to the predictor.\n",
    "- Then this codeblock defines a cutoff value, which in the case of nouns is 0.47.\n",
    "- Anything above 0.47 will be predicted as a noun, anything below won't. So in the example, 0.62 passes.\n",
    "- If a word passes, the function also calls the noun decomposer to return the predicted morphemes of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_morphemes_dict(word_type, m):\n",
    "    if len(m['morphemes']) == 0:\n",
    "        return word_type\n",
    "    else:\n",
    "        return str(word_type) + ', stem: ' + str(m['stem']) + ', morphemes: ' + ', '.join(m['morphemes'])\n",
    "\n",
    "def predict_word_type(w):\n",
    "    \n",
    "    pred_list = []\n",
    "    \n",
    "    #### exact matches ####\n",
    "    \n",
    "    # article\n",
    "    if w in ['a', 'az']:\n",
    "        pred_list.append('ART<DEF>')\n",
    "    elif w == 'egy':\n",
    "        pred_list.append('ART<INDEF>')\n",
    "    \n",
    "    # preverb\n",
    "    if w in prevs_list:\n",
    "        pred_list.append('PREV')\n",
    "    \n",
    "    #### set-similarities ####\n",
    "    \n",
    "    # adverb\n",
    "    if w in adverbs_list:\n",
    "        pred_list.append('ADV')\n",
    "    elif set_similarity(w, adverbs_list) > 0.01:\n",
    "        pred_list.append('ADV')\n",
    "    \n",
    "    # num\n",
    "    is_num = False\n",
    "    if w in all_nums_list:\n",
    "        is_num = True\n",
    "    elif set_similarity(w, all_nums_list) > 0.02:\n",
    "        is_num = True\n",
    "    \n",
    "    # conjunction\n",
    "    if w in conjs_list:\n",
    "        pred_list.append('CONJ')\n",
    "    elif set_similarity(w, conjs_list) > 0.01:\n",
    "        pred_list.append('CONJ')\n",
    "    \n",
    "    # determiners\n",
    "    is_determiner = False\n",
    "    if w in dets_list:\n",
    "        is_determiner = True\n",
    "    elif set_similarity(w, dets_list) > 0.01:\n",
    "        is_determiner = True\n",
    "    \n",
    "    # onomatopoeia\n",
    "    if w in onos_list:\n",
    "        pred_list.append('ONO')\n",
    "    elif set_similarity(w, onos_list) > 0.0158:\n",
    "        pred_list.append('ONO')\n",
    "    \n",
    "    # postposition\n",
    "    if w in postps_list:\n",
    "        pred_list.append('POSTP')\n",
    "    elif set_similarity(w, postps_list) > 0.15:\n",
    "        pred_list.append('POSTP')\n",
    "    \n",
    "    # preposition\n",
    "    if w in preps_list:\n",
    "        pred_list.append('PREP')\n",
    "    elif set_similarity(w, preps_list) > 0.25:\n",
    "        pred_list.append('PREP')\n",
    "    \n",
    "    # utterance / interjection\n",
    "    if w in utt_ints_list:\n",
    "        pred_list.append('UTT-INT')\n",
    "    elif set_similarity(w, utt_ints_list) > 0.027:\n",
    "        pred_list.append('UTT-INT')\n",
    "    \n",
    "    #### NNs ####\n",
    "    \n",
    "    # word type\n",
    "    types = predict_basic_word_type(w)\n",
    "    \n",
    "    # adjective\n",
    "    if types[0] > 0.115:\n",
    "        adj_dict = morphemes_adjective(w)\n",
    "        pred_list.append(expand_morphemes_dict('ADJ', adj_dict))\n",
    "    \n",
    "    # determiner\n",
    "    if is_determiner or types[1] > 0.006:\n",
    "        det_dict = morphemes_determiner(w)\n",
    "        pred_list.append(expand_morphemes_dict('DET', det_dict))\n",
    "    \n",
    "    # noun\n",
    "    if types[2] > 0.47:\n",
    "        noun_dict = morphemes_noun(w)\n",
    "        pred_list.append(expand_morphemes_dict('NOUN', noun_dict))\n",
    "    \n",
    "    # num\n",
    "    if is_num or types[3] > 0.002:\n",
    "        num_dict = morphemes_num(w)\n",
    "        pred_list.append('NUM, type: ' + ', '.join(num_dict['morphemes']))\n",
    "    \n",
    "    # verb\n",
    "    if types[4] > 0.5:\n",
    "        verb_dict = morphemes_verb(w)\n",
    "        pred_list.append(expand_morphemes_dict('VERB', verb_dict))\n",
    "    \n",
    "    if len(pred_list) == 0:\n",
    "        \n",
    "        word_type_list = ['ADJ', 'DET', 'NOUN', 'NUM', 'VERB']\n",
    "        t = word_type_dict[np.argmax(types)]\n",
    "        \n",
    "        if t == 'ADJ':\n",
    "            m_dict = morphemes_adjective(w)\n",
    "        elif t == 'DET':\n",
    "            m_dict = morphemes_determiner(w)\n",
    "        elif t == 'NOUN':\n",
    "            m_dict = morphemes_noun(w)\n",
    "        elif t == 'NUM':\n",
    "            m_dict = morphemes_num(w)\n",
    "        elif t == 'VERB':\n",
    "            m_dict = morphemes_verb(w)\n",
    "        \n",
    "        pred_list.append(expand_morphemes_dict(t, m_dict))\n",
    "    \n",
    "    return '\\n'.join(pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** Example predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB, stem: csinál, morphemes: DEF, MODAL\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('csinálhatja'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB, stem: csinált, morphemes: PAST, PERS<2>, PLUR\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('csináltatok'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN, stem: házait, morphemes: CAS<INE>, PLUR\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('házaitokban'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM, type: COUNT\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('száz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONO\n",
      "NOUN, stem: blablab, morphemes: CAS<ILL>\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('blablabla'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB, stem: szép, morphemes: PERS<1>\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('szépek'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('ülve'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n",
      "NUM, type: COUNT\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('őz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONJ\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('és'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n",
      "NUM, type: COUNT\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('többször'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV\n",
      "CONJ\n",
      "ADJ\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('elvétve'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('összegyűlik'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB, stem: megcsinál, morphemes: MODAL\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('megcsinálhat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB, stem: megcsinálgat, morphemes: DEF, PAST, PERS<1>, PLUR\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('megcsinálgattuk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN, stem: kutyá, morphemes: CAS<DAT>, PLUR\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('kutyáinknak'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN, stem: kutyát, morphemes: CAS<SBL>, PLUR\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('kutyátokra'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('napsugár'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('napnál'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('naphoz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('napoz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('nappal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN, stem: napja, morphemes: CAS<INE>, PLUR, POSS<1><PLUR>\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('napjainkban'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJ, stem: napozó, morphemes: CAS<ABL>, PLUR\n",
      "NOUN, stem: napozó, morphemes: CAS<ABL>, PLUR\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('napozóktól'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN, stem: napj, morphemes: CAS<ELA>\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('napjukból'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('falat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('halat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJ\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('kutat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONJ\n",
      "NOUN\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('avagy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV\n",
      "ADJ\n",
      "DET, stem: ugyani, morphemes: CAS<SUE>\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('ugyanilyen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB, stem: fogad, morphemes: COND, DEF, PERS<2>, PLUR\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('fogadnátok'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV\n",
      "ADJ\n",
      "DET\n",
      "NOUN, stem: ugyanat, morphemes: CAS<ABL>\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('ugyanattól'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n",
      "NUM, type: ITER\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('százkilencszer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB, stem: megtanul, morphemes: MODAL, PAST, PERS<2>, PLUR\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('megtanulhattatok'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB, stem: kinyújtóz, morphemes: PAST, PLUR\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('kinyújtóztak'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB, stem: megszentségtelen, morphemes: DEF\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('megszentségtelenít'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJ\n"
     ]
    }
   ],
   "source": [
    "print(predict_word_type('átlós'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
